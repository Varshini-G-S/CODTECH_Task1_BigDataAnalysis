# CODTECH INTERNSHIP - TASK 1 : BIG DATA ANALYSIS (REAL DATA)
# BY VARSHINI G S

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count, desc
import time

# 1ï¸âƒ£ Start Spark session
spark = SparkSession.builder.appName("RealData_BigDataAnalysis").getOrCreate()
print("\nğŸš€ Spark Session Started")

# 2ï¸âƒ£ Load your real dataset (uploaded CSV file)
data_path = "Electric_Vehicle_Population_Data.csv"  # use your file name here

print(f"\nğŸ“‚ Loading real dataset: {data_path}")
start = time.time()

df = spark.read.option("header", True).option("inferSchema", True).csv(data_path)

print(f"âœ… Dataset loaded in {round(time.time() - start, 2)} sec")
print(f"Total Rows: {df.count()}\n")

# 3ï¸âƒ£ Show sample rows and schema
print("ğŸ“Š Sample Data:")
df.show(5)
print("\nğŸ“˜ Schema Information:")
df.printSchema()

# 4ï¸âƒ£ Clean data: drop missing rows and select numeric columns
print("\nğŸ§¹ Cleaning data...")
numeric_cols = [field.name for field in df.schema.fields if str(field.dataType) in ("IntegerType", "DoubleType", "LongType")]
if len(numeric_cols) >= 2:
    df_clean = df.select(*numeric_cols).dropna()
else:
    df_clean = df.na.drop()

print(f"âœ… Cleaned dataset has {df_clean.count()} rows\n")

# 5ï¸âƒ£ Basic big data analysis
print("ğŸ“ˆ Running basic analysis...")

# Find average of each numeric column (if applicable)
summary = df_clean.select(
    *[avg(col(c)).alias(f"avg_{c}") for c in df_clean.columns if df_clean.schema[c].dataType.simpleString() in ["int", "double", "long"]]
)

# Count total records
record_count = df_clean.count()

print("\nâœ… Basic Summary Statistics:")
summary.show()

# 6ï¸âƒ£ Example aggregation (if 'category' or 'id' columns exist)
cols = [c.lower() for c in df_clean.columns]
if "category" in cols:
    print("\nğŸ”¹ Aggregation by 'category':")
    result = df_clean.groupBy("category").agg(
        count("*").alias("Total_Records"),
        *[avg(col(c)).alias(f"avg_{c}") for c in df_clean.columns if c != "category"]
    ).orderBy(desc("Total_Records"))
    result.show(10)
else:
    print("\nâš ï¸ No 'category' column found. Showing overall summary only.")

# 7ï¸âƒ£ Save results to CSV file
output_path = "output/task1_results"
summary.write.mode("overwrite").option("header", True).csv(output_path)
print(f"\nğŸ’¾ Results saved to folder: {output_path}")

# 8ï¸âƒ£ Stop Spark
spark.stop()
print("\nğŸ Spark session stopped. Task 1 Completed Successfully!")
